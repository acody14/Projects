{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c599e28-d903-458c-a0d1-885ada419c88",
   "metadata": {},
   "source": [
    "# Regex/Text Preprocessing Practice for Austin\n",
    "### Basic Substitutions with re.sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6d6d12f-4e99-4b9f-8022-8ba3aba5aa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"<p>    This is a paragraph</p>\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b22eaa19-5209-4d19-983d-bff89490d8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>    This is a paragraph</p>\n",
      "    This is a paragraph\n",
      "This is a paragraph\n"
     ]
    }
   ],
   "source": [
    "#removing html tags from text\n",
    "\n",
    "#format  re.sub(pattern, replacement_text, input)\n",
    "cleaned_text = re.sub(r'</?p>', '', text)\n",
    "#removing extra whitespace from text\n",
    "cleaner_text = re.sub('\\s{4}','', cleaned_text)\n",
    "print(text)\n",
    "print(cleaned_text)\n",
    "print(cleaner_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95cc42d8-fbbe-424e-b67b-2cb0f9e25c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>Nation's Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini</h1>\n",
      "Nation's Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini\n"
     ]
    }
   ],
   "source": [
    "headline_one = '<h1>Nation\\'s Top Pseudoscientists Harness High-Energy Quartz Crystal Capable Of Reversing Effects Of Being Gemini</h1>'\n",
    "headline_no_tag = re.sub(r'</?h1>', '', headline_one)\n",
    "print(headline_one)\n",
    "print(headline_no_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24f38023-62ea-42e6-803d-8d6140601cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@fat_meats, veggies are better than you think.\n",
      "fat_meats, veggies are better than you think.\n"
     ]
    }
   ],
   "source": [
    "tweet = '@fat_meats, veggies are better than you think.'\n",
    "tweet_no_at = re.sub(r'@', '', tweet)\n",
    "print(tweet)\n",
    "print(tweet_no_at)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccad4ff-fd2b-4bfa-88a2-085506c742c7",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22e96520-8886-443f-9eb5-8b3b8e936902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word tokenized text:\n",
      "['An', 'electrocardiogram', 'is', 'used', 'to', 'record', 'the', 'electrical', 'conduction', 'through', 'a', 'person', \"'s\", 'heart', '.', 'The', 'readings', 'can', 'be', 'used', 'to', 'diagnose', 'cardiac', 'arrhythmias', '.']\n",
      "\n",
      "sentence tokenized text:\n",
      "[\"An electrocardiogram is used to record the electrical conduction through a person's heart.\", 'The readings can be used to diagnose cardiac arrhythmias.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "ecg_text = 'An electrocardiogram is used to record the electrical conduction through a person\\'s heart. The readings can be used to diagnose cardiac arrhythmias.'\n",
    "\n",
    "wt_ecg_text = word_tokenize(ecg_text)\n",
    "st_ecg_text = sent_tokenize(ecg_text)\n",
    "\n",
    "print(\"word tokenized text:\")\n",
    "print(wt_ecg_text)\n",
    "print(\"\\nsentence tokenized text:\")\n",
    "print(st_ecg_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6a5c9-afb5-4b06-9db1-db15ccafbd38",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Normalization is a general catch-all term for some preprocessing tasks including:\n",
    "- changing case\n",
    "- stopword removal\n",
    "- stemming (removing prefixes and suffixes)\n",
    "- lemmatizing (changing a word to its root form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87cf59c1-0e6f-43bf-9523-60b5d6952030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing case\n",
    "\n",
    "brands = 'Salvation Army, YMCA, Boys & Girls Club of America'\n",
    "\n",
    "brands_lower = brands.lower()\n",
    "brands_upper = brands.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1536e146-63c6-46b2-9005-b8e94a531513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'YouGov', 'study', 'found', 'American', \"'s\", 'like', 'Italian', 'food', 'country', \"'s\", 'cuisine', '.']\n"
     ]
    }
   ],
   "source": [
    "# stopword removal\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "survey_text = 'A YouGov study found that American\\'s like Italian food more than any other country\\'s cuisine.'\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# creating tokenized survey text and then filtering stop words from the tokenized text\n",
    "tokenized_text = word_tokenize(survey_text)\n",
    "filtered_text = [word for word in tokenized_text if word not in stop_words]\n",
    "\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74465b42-7cd5-4d59-ac23-d5d45e60a030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['java', 'is', 'an', 'indonesian', 'island', 'in', 'the', 'pacif', 'ocean', '.', 'it', 'is', 'the', 'most', 'popul', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'peopl', '.']\n"
     ]
    }
   ],
   "source": [
    "# stemming (removing prefixes and suffixes)\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "populated_island = 'Java is an Indonesian island in the Pacific Ocean. It is the most populated island in the world, with over 140 million people.'\n",
    "\n",
    "# instantiating stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# tokenizing text\n",
    "tokenized_text = word_tokenize(populated_island)\n",
    "\n",
    "# using our stemmer to stem the text\n",
    "stemmed_text = [stemmer.stem(word) for word in tokenized_text]\n",
    "\n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb695ba3-e2c7-4d4e-98ea-0c006302e776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Java', 'is', 'an', 'Indonesian', 'island', 'in', 'the', 'Pacific', 'Ocean', '.', 'It', 'is', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "# lemmatizing (changing a word to its root form)\n",
    "# (without part of speech tagging first, see below for lemmatization with pos tagging)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "populated_island = 'Java is an Indonesian island in the Pacific Ocean. It is the most populated island in the world, with over 140 million people.'\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenized_text = word_tokenize(populated_island)\n",
    "\n",
    "lemmatized_text = [lemmatizer.lemmatize(word) for word in tokenized_text]\n",
    "\n",
    "print(lemmatized_text)\n",
    "\n",
    "######################\n",
    "# Note how this isn't very useful -- it just changes a little bit\n",
    "# that's because we need to use part of speech tagging\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "63cf3dc2-3742-42d9-ac9b-0c07015d7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of Speech Tagging function\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  \n",
    "  pos_counts = Counter()\n",
    "\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "79fb2f0c-b570-442d-8375-4513e756f2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Java', 'be', 'an', 'Indonesian', 'island', 'in', 'the', 'Pacific', 'Ocean', '.', 'It', 'be', 'the', 'most', 'populate', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
     ]
    }
   ],
   "source": [
    "# lemmatizing WITH part of speech tagging\n",
    "# combining previous two steps to improve our lemmatization\n",
    "\n",
    "pos_lemmatized = [lemmatizer.lemmatize(word, get_part_of_speech(word)) for word in tokenized_text]\n",
    "\n",
    "print(pos_lemmatized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
