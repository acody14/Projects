{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503aa2ee-cce4-437e-995f-d9fa3549e17f",
   "metadata": {},
   "source": [
    "# Language Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec715ba-1a5f-4bb5-b011-8d956848db85",
   "metadata": {},
   "source": [
    "## Compiling and Matching\n",
    "- .compile() turns your regex string into a regular expression object\n",
    "- .match() will only find matches at the start of a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0ac4092-6fe1-4a12-89d0-bfeed753394c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dorothy\n"
     ]
    }
   ],
   "source": [
    "#here we are compiling a regular expression and matching to a string\n",
    "import re\n",
    "\n",
    "# characters are defined\n",
    "character_1 = \"Dorothy\"\n",
    "\n",
    "# compile your regular expression here\n",
    "regular_expression = re.compile(\"\\w{7}\")\n",
    "\n",
    "# check for a match to character_1 here\n",
    "result_1 = regular_expression.match(character_1)\n",
    "\n",
    "# store and print the matched text here\n",
    "match_1 = result_1.group(0)\n",
    "print(match_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ddda5f-4801-4a3f-8619-56dcf6ab565a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match object: <re.Match object; span=(0, 7), match='Dorothy'>\n",
      "match:        Dorothy\n"
     ]
    }
   ],
   "source": [
    "# Same thing can be done in less code (combining match and compile steps by using re's .match() method)\n",
    "character_1 = \"Dorothy\"\n",
    "\n",
    "result_1 = re.match(\"\\w{7}\", character_1)\n",
    "print(\"match object:\", result_1)\n",
    "      \n",
    "match_1 = result_1.group(0)\n",
    "print(\"match:       \",match_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "932a8ad9-b131-4a97-aacb-dcf2e1e0cb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match object: None\n"
     ]
    }
   ],
   "source": [
    "# Notice how this code won't match for something shorter than seven characters:\n",
    "character_2 = \"Henry\"\n",
    "\n",
    "result_2 = re.match(\"\\w{7}\", character_2)\n",
    "print(\"match object:\",result_2)\n",
    "      \n",
    "#match_2 = result_2.group(0)\n",
    "#print(match_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f475059-68d8-4fbf-b0e1-c6d82842f653",
   "metadata": {},
   "source": [
    "## Searching and Finding\n",
    "- .match() will only find matches at the start of a string\n",
    "- .search() will look left to right through an entire piece of text and return a match object for the first match to the regular expression given\n",
    "- .findall() will return a list of all non-overlapping matches of the regular expression in the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a2ba97-d310-45ac-87af-f3c1b8bb8e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wizard\n"
     ]
    }
   ],
   "source": [
    "# import L. Frank Baum's The Wonderful Wizard of Oz\n",
    "oz_text = open(\"the_wizard_of_oz_text.txt\",encoding='utf-8').read().lower()\n",
    "\n",
    "# search oz_text for an occurrence of 'wizard' here\n",
    "found_wizard = re.search(\"wizard\", oz_text)\n",
    "print(found_wizard.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bdc08b4-6f04-4e11-871c-3835c089da20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion', 'lion']\n",
      "181\n"
     ]
    }
   ],
   "source": [
    "# find all the occurrences of 'lion' in oz_text here\n",
    "all_lions = re.findall(\"lion\", oz_text)\n",
    "print(all_lions)\n",
    "\n",
    "# store and print the length of all_lions here\n",
    "number_lions = len(all_lions)\n",
    "print(number_lions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc2fa5-e123-4b7a-b636-4a22afe453f5",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "\n",
    "We start by sentence tokenizing the text, followed by word tokenizing each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "865f1d47-99ea-444e-9bd4-29bed5858583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 95\n",
      "['she', 'was', 'the', 'wicked', 'witch', 'of', 'the', 'east', 'as', 'i', 'said', 'answered', 'the', 'little', 'woman'] \n",
      "\n",
      "Sentence 96\n",
      "['she', 'has', 'held', 'all', 'the', 'munchkins', 'in', 'bondage', 'for', 'many', 'years', 'making', 'them', 'slave', 'for', 'her', 'night', 'and', 'day'] \n",
      "\n",
      "Sentence 97\n",
      "['now', 'they', 'are', 'all', 'set', 'free', 'and', 'are', 'grateful', 'to', 'you', 'for', 'the', 'favor', 'who', 'are', 'the', 'munchkins', 'inquired', 'dorothy'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# first we SENTENCE tokenize the text\n",
    "sent_tokenized_oz = sent_tokenize(oz_text)\n",
    "\n",
    "# cleaning each sentence\n",
    "cleaned_sent_oz = []\n",
    "for sentence in sent_tokenized_oz:\n",
    "    cleaned_sent_oz.append(re.sub('\\W+', ' ', sentence).lower())\n",
    "\n",
    "# then we WORD tokenize each sentence\n",
    "word_tokenized_oz = []\n",
    "for sentence in cleaned_sent_oz:\n",
    "    word_tokenized_oz.append(word_tokenize(sentence))\n",
    "\n",
    "# checking sentences --> Now word_tokenized_oz is a list of sentences each of which is a list of words!\n",
    "i = 95\n",
    "while i < 98:\n",
    "    print(f\"Sentence {i}\")\n",
    "    print(word_tokenized_oz[i], \"\\n\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cb3a275-968a-488e-ab93-ad62c8c58d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('she', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN'), ('of', 'IN'), ('the', 'DT'), ('east', 'NN'), ('as', 'IN'), ('i', 'NN'), ('said', 'VBD'), ('answered', 'VBD'), ('the', 'DT'), ('little', 'JJ'), ('woman', 'NN')]\n",
      "[('she', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN'), ('of', 'IN'), ('the', 'DT'), ('east', 'NN'), ('as', 'IN'), ('i', 'NN'), ('said', 'VBD'), ('answered', 'VBD'), ('the', 'DT'), ('little', 'JJ'), ('woman', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Part of Speech Tagging\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "\n",
    "pos_tagged_oz = pos_tag(word_tokenized_oz[95])\n",
    "print(pos_tagged_oz)\n",
    "\n",
    "pos_tagged_oz = []\n",
    "for sentence in word_tokenized_oz:\n",
    "    pos_tagged_oz.append(pos_tag(sentence))\n",
    "\n",
    "print(pos_tagged_oz[95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bd714f-00f0-4bfe-bde6-7d05b53b38ae",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "- grouping words by their part of speech\n",
    "- With chunking in nltk, you can define a pattern of parts-of-speech tags using a modified notation of regular expressions:\n",
    "  - `chunk_grammar = \"AN: {<JJ><NN>}\"` with `JJ` being an adjective and `NN` being a noun\n",
    "- You can then find non-overlapping matches, or chunks of words, in the part-of-speech tagged sentences of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af9978ff-478a-4781-a328-37190c1d51aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  she/PRP\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  (AN wicked/JJ witch/NN)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  east/NN\n",
      "  as/IN\n",
      "  i/NN\n",
      "  said/VBD\n",
      "  answered/VBD\n",
      "  the/DT\n",
      "  (AN little/JJ woman/NN))\n",
      "                                                   S                                                                             \n",
      "    _______________________________________________|__________________________________________________________________            \n",
      "   |       |      |      |     |       |      |    |      |          |         |               AN                     AN         \n",
      "   |       |      |      |     |       |      |    |      |          |         |         ______|_____           ______|_____      \n",
      "she/PRP was/VBD the/DT of/IN the/DT east/NN as/IN i/NN said/VBD answered/VBD the/DT wicked/JJ     witch/NN little/JJ     woman/NN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser, Tree\n",
    "\n",
    "# define adjective-noun chunk grammar here\n",
    "chunk_grammar = \"AN:{<JJ><NN>}\"\n",
    "\n",
    "# create RegexpParser object here\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# chunk the pos-tagged sentence at index 95 in pos_tagged_oz here\n",
    "scaredy_cat = chunk_parser.parse(pos_tagged_oz[95])\n",
    "print(scaredy_cat)\n",
    "\n",
    "# pretty_print the chunked sentence here\n",
    "Tree.fromstring(str(scaredy_cat)).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca23b5f-df2f-4355-a3b3-04ecb9576437",
   "metadata": {},
   "source": [
    "## Noun Phrase Chunking\n",
    "- NP chunking is linguistically helpful for determining meaning and bias in a piece of text\n",
    "- A popular form of noun phrase begins with a determiner `DT`, which specifies the noun being referenced, followed by any number of adjectives `JJ`, which describe the noun, and ends with a noun `NN`.\n",
    "  - `chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"`\n",
    "- Chunks are NON-overlapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75b6b215-e820-44e1-996e-a65b17244e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                             S                                                               \n",
      "    _________________________________________________________|_________________________________________________               \n",
      "   |       |      |     |      |          |                  NP                     NP          NP             NP            \n",
      "   |       |      |     |      |          |          ________|________         _____|_____      |      ________|________      \n",
      "she/PRP was/VBD of/IN as/IN said/VBD answered/VBD the/DT wicked/JJ witch/NN the/DT     east/NN i/NN the/DT little/JJ woman/NN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser\n",
    "\n",
    "# define noun-phrase chunk grammar here\n",
    "chunk_grammar = \"NP:{<DT>?<JJ>?<NN>}\"\n",
    "\n",
    "# create RegexpParser object here\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# parsing for noun phrase\n",
    "sentence_95 = chunk_parser.parse(pos_tagged_oz[95])\n",
    "Tree.fromstring(str(sentence_95)).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13100279-a06e-4d50-8b08-4c4dfbe62a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count most common np chunks\n",
    "from collections import Counter\n",
    "\n",
    "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
    "def np_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract noun phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21a91d20-488f-4df6-90d3-0f2470d3a3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((('i', 'NN'),), 293)\n",
      "((('the', 'DT'), ('scarecrow', 'NN')), 212)\n",
      "((('dorothy', 'NN'),), 157)\n",
      "((('the', 'DT'), ('lion', 'NN')), 147)\n",
      "((('the', 'DT'), ('tin', 'NN')), 122)\n",
      "((('woodman', 'NN'),), 113)\n",
      "((('gutenberg', 'NN'),), 74)\n",
      "((('oz', 'NN'),), 69)\n",
      "((('toto', 'NN'),), 67)\n",
      "((('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN')), 57)\n",
      "((('the', 'DT'), ('woodman', 'NN')), 57)\n",
      "((('head', 'NN'),), 55)\n",
      "((('the', 'DT'), ('emerald', 'JJ'), ('city', 'NN')), 50)\n",
      "((('the', 'DT'), ('witch', 'NN')), 49)\n",
      "((('the', 'DT'), ('girl', 'NN')), 46)\n"
     ]
    }
   ],
   "source": [
    "# create a list to hold noun-phrase chunked sentences\n",
    "np_chunked_oz = []\n",
    "\n",
    "# create a for loop through each pos-tagged sentence in pos_tagged_oz here\n",
    "for sentence in pos_tagged_oz:\n",
    "  # chunk each sentence and append to np_chunked_oz here\n",
    "  np_chunked_oz.append(chunk_parser.parse(sentence))\n",
    "\n",
    "# store and print the most common np-chunks here\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_oz)\n",
    "for np in most_common_np_chunks:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed07a136-7757-4838-91b6-6ad6bffc47d0",
   "metadata": {},
   "source": [
    "## Verb Phrase Chunking\n",
    " - A verb phrase is a phrase that contains a verb and its complements, objects, or modifiers\n",
    " - Verb phrases can take a variety of structures, and here you will consider two:\n",
    "    1. The first structure begins with a verb `VB` of any tense, followed by a noun phrase, and ends with an optional adverb `RB` of any form.\n",
    "        * `chunk_grammar = \"VP: {<VB.*><DT>?<JJ>*<NN><RB.?>?}\"`\n",
    "        * This would match `(('said', 'VBD'), ('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN'))`\n",
    "   2. The second structure switches the order of the verb and the noun phrase, but also ends with an optional adverb.\n",
    "        * `chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"`\n",
    "        * This would match `(('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN'), ('said', 'VBD'))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4be575b4-02ae-4ded-9ec7-da032c03174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first defining function for counting verb phrases\n",
    "from collections import Counter\n",
    "\n",
    "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
    "def vp_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract verb phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'VP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "            \n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ea9beed-dbe7-4589-862c-a4b99db398a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((('said', 'VBD'), ('the', 'DT'), ('scarecrow', 'NN')), 33)\n",
      "((('said', 'VBD'), ('the', 'DT'), ('tin', 'NN')), 19)\n",
      "((('said', 'VBD'), ('the', 'DT'), ('lion', 'NN')), 15)\n",
      "((('said', 'VBD'), ('dorothy', 'NN')), 11)\n",
      "((('said', 'VBD'), ('the', 'DT'), ('girl', 'NN')), 10)\n",
      "((('asked', 'VBD'), ('the', 'DT'), ('scarecrow', 'NN')), 10)\n",
      "((('said', 'VBD'), ('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')), 8)\n",
      "((('said', 'VBD'), ('oz', 'NN')), 8)\n",
      "((('pass', 'VB'), ('the', 'DT'), ('night', 'NN')), 6)\n",
      "((('asked', 'VBD'), ('the', 'DT'), ('girl', 'NN')), 6)\n",
      "((('don', 'VBP'), ('t', 'NN')), 6)\n",
      "((('set', 'VBN'), ('forth', 'NN')), 6)\n",
      "((('asked', 'VBN'), ('dorothy', 'NN')), 5)\n",
      "((('answered', 'VBD'), ('the', 'DT'), ('scarecrow', 'NN')), 5)\n",
      "((('thought', 'VBD'), ('i', 'NN')), 5)\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser\n",
    "\n",
    "###################\n",
    "# FIRST STRUCTURE #\n",
    "###################\n",
    "\n",
    "# define verb phrase chunk grammar here\n",
    "chunk_grammar = \"VP:{<VB.?><DT>?<JJ>*<NN><RB.?>?}\"\n",
    "\n",
    "# create RegexpParser object here\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# create a list to hold verb-phrase chunked sentences\n",
    "vp_chunked_oz = list()\n",
    "\n",
    "# create for loop through each pos-tagged sentence in pos_tagged_oz here\n",
    "for sentence in pos_tagged_oz:\n",
    "  # chunk each sentence and append to vp_chunked_oz here\n",
    "  vp_chunked_oz.append(chunk_parser.parse(sentence))\n",
    "\n",
    "# store and print the most common vp-chunks here\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_oz)\n",
    "for chunk in most_common_vp_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fa4aa9e-76c4-489d-9fbe-aef818a08bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((('i', 'NN'), ('am', 'VBP')), 31)\n",
      "((('i', 'NN'), ('was', 'VBD')), 17)\n",
      "((('dorothy', 'NN'), ('was', 'VBD')), 14)\n",
      "((('i', 'NN'), ('had', 'VBD')), 8)\n",
      "((('project', 'NN'), ('gutenberg', 'VBZ')), 8)\n",
      "((('i', 'NN'), ('know', 'VBP')), 7)\n",
      "((('dorothy', 'NN'), ('had', 'VBD')), 7)\n",
      "((('oz', 'NN'), ('was', 'VBD')), 6)\n",
      "((('i', 'NN'), ('want', 'VBP')), 6)\n",
      "((('oz', 'NN'), ('had', 'VBD')), 6)\n",
      "((('toto', 'NN'), ('did', 'VBD'), ('not', 'RB')), 5)\n",
      "((('dorothy', 'NN'), ('looked', 'VBD')), 5)\n",
      "((('i', 'NN'), ('have', 'VBP'), ('never', 'RB')), 5)\n",
      "((('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN'), ('had', 'VBD')), 5)\n",
      "((('i', 'NN'), ('have', 'VBP')), 5)\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# SECOND STRUCTURE #\n",
    "###################\n",
    "\n",
    "# define verb phrase chunk grammar here\n",
    "chunk_grammar = \"VP:{<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
    "\n",
    "# create RegexpParser object here\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# create a list to hold verb-phrase chunked sentences\n",
    "vp_chunked_oz = list()\n",
    "\n",
    "# create for loop through each pos-tagged sentence in pos_tagged_oz here\n",
    "for sentence in pos_tagged_oz:\n",
    "  # chunk each sentence and append to vp_chunked_oz here\n",
    "  vp_chunked_oz.append(chunk_parser.parse(sentence))\n",
    "\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_oz)\n",
    "for chunk in most_common_vp_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786ba383-9b84-4731-ae60-cedb15dede4a",
   "metadata": {},
   "source": [
    "## Chunk Filtering\n",
    "* You can alternateively define Noun Phrases (or whatever kind of phrase you want) by filtering out verbs and prepositions to leave only Noun Phrase contents\n",
    "* `chunk_grammar = \"NP: {<.*>+}}<VB.?|IN>+{\"`\n",
    "    * `{<.*>+}` matches every part of speech\n",
    "    * `}<VB.?|IN>+{` excludes verbs or prepositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cc4a1b8-ea40-4762-8d67-b6a62290ce4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk\n",
      "    she/PRP\n",
      "    was/VBD\n",
      "    the/DT\n",
      "    wicked/JJ\n",
      "    witch/NN\n",
      "    of/IN\n",
      "    the/DT\n",
      "    east/NN\n",
      "    as/IN\n",
      "    i/NN\n",
      "    said/VBD\n",
      "    answered/VBD\n",
      "    the/DT\n",
      "    little/JJ\n",
      "    woman/NN))\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser, Tree\n",
    "\n",
    "# See how it's all one big chunk\n",
    "\n",
    "# define chunk grammar to chunk an entire sentence together\n",
    "grammar = \"Chunk: {<.*>+}\"\n",
    "\n",
    "# create RegexpParser object\n",
    "parser = RegexpParser(grammar)\n",
    "\n",
    "# chunk the pos-tagged sentence at index 230 in pos_tagged_oz\n",
    "my_chunks = parser.parse(pos_tagged_oz[95])\n",
    "print(my_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "392ea66e-56ac-4a3c-909f-e7642d4e2000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP she/PRP)\n",
      "  was/VBD\n",
      "  (NP the/DT wicked/JJ witch/NN)\n",
      "  of/IN\n",
      "  (NP the/DT east/NN)\n",
      "  as/IN\n",
      "  (NP i/NN)\n",
      "  said/VBD\n",
      "  answered/VBD\n",
      "  (NP the/DT little/JJ woman/NN))\n",
      "                                                             S                                                               \n",
      "    _________________________________________________________|_________________________________________________               \n",
      "   |      |     |      |          |          NP              NP                     NP          NP             NP            \n",
      "   |      |     |      |          |          |       ________|________         _____|_____      |      ________|________      \n",
      "was/VBD of/IN as/IN said/VBD answered/VBD she/PRP the/DT wicked/JJ witch/NN the/DT     east/NN i/NN the/DT little/JJ woman/NN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define noun phrase chunk grammar using chunk filtering here\n",
    "chunk_grammar = \"\"\"NP: {<.*>+}\n",
    "                       }<VB.?|IN>+{\"\"\"\n",
    "\n",
    "# create RegexpParser object here\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# chunk and filter the pos-tagged sentence at index 95 in pos_tagged_oz here\n",
    "filtered_dancers = chunk_parser.parse(pos_tagged_oz[95])\n",
    "print(filtered_dancers)\n",
    "\n",
    "\n",
    "# pretty_print the chunked and filtered sentence here\n",
    "Tree.fromstring(str(filtered_dancers)).pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
